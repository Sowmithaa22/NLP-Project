{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install whisper"
      ],
      "metadata": {
        "id": "ZEAgfAI20897",
        "outputId": "7c863cdb-7c7b-4b2c-c67c-14ce16d3725b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting whisper\n",
            "  Downloading whisper-1.1.10.tar.gz (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from whisper) (1.17.0)\n",
            "Building wheels for collected packages: whisper\n",
            "  Building wheel for whisper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for whisper: filename=whisper-1.1.10-py3-none-any.whl size=41120 sha256=bffa77e9f3e572ff9eb18fb0fa32ae1642541449aaa2658ac7084c64a899123b\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/65/ee/4e6672aabfa486d3341a39a04f8f87c77e5156149299b5a7d0\n",
            "Successfully built whisper\n",
            "Installing collected packages: whisper\n",
            "Successfully installed whisper-1.1.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import torch\n",
        "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "def transcribe_audio(audio_path):\n",
        "    \"\"\"\n",
        "    Transcribe audio to text using Whisper model.\n",
        "\n",
        "    Supported models:\n",
        "    - tiny: Fastest, lowest quality\n",
        "    - base: Small size, moderate quality\n",
        "    - small: Balanced performance\n",
        "    - medium: High quality\n",
        "    - large: Best accuracy, most compute-intensive\n",
        "    \"\"\"\n",
        "    model = whisper.load_model(\"base\")  # Choose model size\n",
        "    result = model.transcribe(audio_path)\n",
        "    return result[\"text\"]\n",
        "\n",
        "def generate_summaries(text):\n",
        "    \"\"\"\n",
        "    Generate summaries using different models.\n",
        "\n",
        "    Summarization Models:\n",
        "    1. BART-based models\n",
        "    2. T5-based models\n",
        "    3. PEGASUS models\n",
        "    \"\"\"\n",
        "    # BART Summarization\n",
        "    bart_summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "    bart_summary = bart_summarizer(text, max_length=130, min_length=30, do_sample=False)[0]['summary_text']\n",
        "\n",
        "    # T5 Summarization\n",
        "    t5_model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
        "    t5_tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "    t5_inputs = t5_tokenizer(\"summarize: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    t5_summary_ids = t5_model.generate(t5_inputs[\"input_ids\"], num_beams=4, max_length=100, early_stopping=True)\n",
        "    t5_summary = t5_tokenizer.decode(t5_summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return {\n",
        "        \"BART Summary\": bart_summary,\n",
        "        \"T5 Summary\": t5_summary\n",
        "    }\n",
        "\n",
        "def main(audio_path):\n",
        "    \"\"\"Main function to process audio and generate summaries\"\"\"\n",
        "    # Transcribe audio\n",
        "    transcribed_text = transcribe_audio(audio_path)\n",
        "    print(\"Transcribed Text:\")\n",
        "    print(transcribed_text)\n",
        "\n",
        "    # Generate summaries\n",
        "    summaries = generate_summaries(transcribed_text)\n",
        "\n",
        "    print(\"\\nSummaries:\")\n",
        "    for model, summary in summaries.items():\n",
        "        print(f\"\\n{model}:\")\n",
        "        print(summary)\n"
      ],
      "metadata": {
        "id": "w1gKiTj20NSu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    main(\"/content/sample.mp3\")"
      ],
      "metadata": {
        "id": "0ug_Alqt02Gl",
        "outputId": "27d983ef-c332-436f-9b52-7728b9f0cf0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'whisper' has no attribute 'load_model'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e02fc76e59c0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/sample.mp3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-81f0c1edf68a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(audio_path)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;34m\"\"\"Main function to process audio and generate summaries\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# Transcribe audio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mtranscribed_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranscribe_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Transcribed Text:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscribed_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-81f0c1edf68a>\u001b[0m in \u001b[0;36mtranscribe_audio\u001b[0;34m(audio_path)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;34m-\u001b[0m \u001b[0mlarge\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBest\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmost\u001b[0m \u001b[0mcompute\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mintensive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \"\"\"\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhisper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"base\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Choose model size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranscribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'whisper' has no attribute 'load_model'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import torch\n",
        "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "def transcribe_audio(audio_path):\n",
        "    \"\"\"\n",
        "    Transcribe audio to text using Whisper model.\n",
        "\n",
        "    Supported models:\n",
        "    - tiny: Fastest, lowest quality\n",
        "    - base: Small size, moderate quality\n",
        "    - small: Balanced performance\n",
        "    - medium: High quality\n",
        "    - large: Best accuracy, most compute-intensive\n",
        "    \"\"\"\n",
        "    model = whisper.load_model('base')  # Correct method for loading Whisper model\n",
        "    result = model.transcribe(audio_path)\n",
        "    return result[\"text\"]\n",
        "\n",
        "def generate_summaries(text):\n",
        "    \"\"\"\n",
        "    Generate summaries using different models.\n",
        "    \"\"\"\n",
        "    # BART Summarization\n",
        "    bart_summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "    bart_summary = bart_summarizer(text, max_length=130, min_length=30, do_sample=False)[0]['summary_text']\n",
        "\n",
        "    # T5 Summarization\n",
        "    t5_model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
        "    t5_tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "    t5_inputs = t5_tokenizer(\"summarize: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    t5_summary_ids = t5_model.generate(t5_inputs[\"input_ids\"], num_beams=4, max_length=100, early_stopping=True)\n",
        "    t5_summary = t5_tokenizer.decode(t5_summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return {\n",
        "        \"BART Summary\": bart_summary,\n",
        "        \"T5 Summary\": t5_summary\n",
        "    }\n",
        "\n",
        "def main(audio_path):\n",
        "    \"\"\"Main function to process audio and generate summaries\"\"\"\n",
        "    # Transcribe audio\n",
        "    transcribed_text = transcribe_audio(audio_path)\n",
        "    print(\"Transcribed Text:\")\n",
        "    print(transcribed_text)\n",
        "\n",
        "    # Generate summaries\n",
        "    summaries = generate_summaries(transcribed_text)\n",
        "\n",
        "    print(\"\\nSummaries:\")\n",
        "    for model, summary in summaries.items():\n",
        "        print(f\"\\n{model}:\")\n",
        "        print(summary)\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    main(\"/content/sample.mp3\")"
      ],
      "metadata": {
        "id": "wbs38pF11CGc",
        "outputId": "d335d673-b267-4231-e43c-6d706b34c138",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'whisper' has no attribute 'load_model'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-aca131dacf1b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/sample.mp3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-aca131dacf1b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(audio_path)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;34m\"\"\"Main function to process audio and generate summaries\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# Transcribe audio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mtranscribed_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranscribe_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Transcribed Text:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscribed_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-aca131dacf1b>\u001b[0m in \u001b[0;36mtranscribe_audio\u001b[0;34m(audio_path)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;34m-\u001b[0m \u001b[0mlarge\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBest\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmost\u001b[0m \u001b[0mcompute\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mintensive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \"\"\"\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhisper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'base'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Correct method for loading Whisper model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranscribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'whisper' has no attribute 'load_model'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade whisper # update whisper to ensure you have the most recent version with the load_model() function\n",
        "\n",
        "# Restart your runtime/kernel at this point"
      ],
      "metadata": {
        "id": "aoAzzQ0m3Zyo",
        "outputId": "5f698fa3-8586-4b8d-b183-3613fd14b1f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: whisper in /usr/local/lib/python3.11/dist-packages (1.1.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from whisper) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Approach 2: Using Google Speech Recognition Model"
      ],
      "metadata": {
        "id": "_lFhjkvO62A5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SpeechRecognition"
      ],
      "metadata": {
        "id": "oxbg67Tv4KWH",
        "outputId": "2b879c0b-d2b7-456a-cc45-cff70e16084c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting SpeechRecognition\n",
            "  Downloading SpeechRecognition-3.14.1-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from SpeechRecognition) (4.12.2)\n",
            "Downloading SpeechRecognition-3.14.1-py3-none-any.whl (32.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: SpeechRecognition\n",
            "Successfully installed SpeechRecognition-3.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import speech_recognition as sr\n",
        "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "def transcribe_audio(audio_path):\n",
        "    \"\"\"\n",
        "    Transcribe audio to text using Google Speech Recognition.\n",
        "    Supports various audio formats (WAV, FLAC, etc.)\n",
        "    \"\"\"\n",
        "    recognizer = sr.Recognizer()\n",
        "\n",
        "    with sr.AudioFile(audio_path) as source:\n",
        "        audio_data = recognizer.record(source)\n",
        "\n",
        "    try:\n",
        "        # Use Google Speech Recognition (requires internet)\n",
        "        text = recognizer.recognize_google(audio_data)\n",
        "        return text\n",
        "    except sr.UnknownValueError:\n",
        "        return \"Google Speech Recognition could not understand audio\"\n",
        "    except sr.RequestError as e:\n",
        "        return f\"Could not request results from Google Speech Recognition service; {e}\"\n",
        "\n",
        "def generate_summaries(text, max_length=150):\n",
        "    \"\"\"\n",
        "    Generate summaries using multiple models.\n",
        "    \"\"\"\n",
        "    summaries = {}\n",
        "\n",
        "    # BART Summarization\n",
        "    bart_summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "    summaries[\"BART\"] = bart_summarizer(\n",
        "        text,\n",
        "        max_length=max_length,\n",
        "        min_length=30,\n",
        "        do_sample=False\n",
        "    )[0]['summary_text']\n",
        "\n",
        "    # T5 Summarization\n",
        "    t5_model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
        "    t5_tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "    t5_inputs = t5_tokenizer(\n",
        "        \"summarize: \" + text,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=512,\n",
        "        truncation=True\n",
        "    )\n",
        "    t5_summary_ids = t5_model.generate(\n",
        "        t5_inputs[\"input_ids\"],\n",
        "        num_beams=4,\n",
        "        max_length=max_length,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    summaries[\"T5\"] = t5_tokenizer.decode(t5_summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return summaries\n",
        "\n",
        "def main(audio_path):\n",
        "    \"\"\"Main function to process audio and generate summaries\"\"\"\n",
        "    # Transcribe audio\n",
        "    transcribed_text = transcribe_audio(audio_path)\n",
        "    print(\"Transcribed Text:\")\n",
        "    print(transcribed_text)\n",
        "\n",
        "    # Generate summaries\n",
        "    summaries = generate_summaries(transcribed_text)\n",
        "\n",
        "    print(\"\\nSummaries:\")\n",
        "    for model, summary in summaries.items():\n",
        "        print(f\"\\n{model} Summary:\")\n",
        "        print(summary)\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    main(\"/content/sample.wav\")"
      ],
      "metadata": {
        "id": "w8l5Kd2U3oUt",
        "outputId": "36361096-e87c-49df-f4c1-03f2df5ba216",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcribed Text:\n",
            "break to end the long night of their captivity Bud 100 years later the Negro still is not free 100 years later and the chains of discrimination 100 years ago\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Your max_length is set to 150, but your input_length is only 32. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=16)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summaries:\n",
            "\n",
            "BART Summary:\n",
            "The Negro still is not free 100 years later and the chains of discrimination 100 years ago. Break to end the long night of their captivity Bud.\n",
            "\n",
            "T5 Summary:\n",
            "the Negro still is not free 100 years later and the chains of discrimination 100 years ago.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OpVdW4su5N22"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
